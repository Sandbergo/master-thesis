\chapter{Background}\label{cha:background}

The following chapter lays the theoretical groundwork for the project. An understanding of linear algebra, numerical optimization, algorithms, and statistics is assumed. \Cref{sec:back_mathprog,sec:back_bnb,ssec:back_mlp} are adapted from the project report \textit{Multi-Layer Perceptrons for Branching in Mixed-Integer Linear Programming} (2020). 


\section{Mathematical Programming}\label{sec:back_mathprog}

This section presents the field of \textit{mathematical programming} at the level relevant for understanding the thesis.
In this work, the terms mathematical programming, numerical optimization, and optimization are used interchangeably. The differences between these stem largely from the different communities who use them. The section will first cover the topic of linear programming (\gls{LP}), then the topic of mixed-integer linear programming (\gls{MILP}), and lastly a section on computational complexity.  


\subsection{Linear Programming}

In mathematical programming, the general linear problem can be stated as \cite{gasse2019exact}:
\begin{align} \label{eq:lp}
    \underset{\mathbf{x}}{\arg \min }\left\{\mathbf{c}^{\top} \mathbf{x} \; \mid \mathbf{A} \mathbf{x} \leq \mathbf{b},\; \mathbf{x} \in \mathbb{R}_+^{n}\right\},
\end{align}
where $ \mathbf{x} \in \mathbb{R}_+^n$ is the variable vector
with the objective coefficient vector $\mathbf{c} \in \mathbb{R}^n $, 
the constraint coefficient matrix $\mathbf{A} \in \mathbb{R}^{m \times n}$
and the constraint right-hand-side vector $\mathbf{b} \in \mathbb{R}^m $.

The size of the problem will be measured by the dimensions of the constraint coefficient matrix $ \mathbf{A} $, where the number of rows and columns corresponds to the number of variables and constraints, respectively.

These problems are convex \cite{wolsey2020integer}, and can be solved by several efficient algorithms. The simplex algorithm can solve problems on this form efficiently, and the same for interior-point methods \cite{nocedal2006numerical}. These algorithms are good average performance but do not have guaranteed polynomial running time in the worst case. Guaranteed polynomial solution algorithms do exist, for instance \textit{Karamkar's algorithm} \cite{karamkar1984new}. 


\subsection{Mixed Integer Linear Programming}

Mixed integer linear programming is a superset of linear programming, where one or more of the variables can be restricted to discrete values. The general problem can in this case be stated as \cite{gasse2019exact}:
\begin{align}\label{eq:milp}
    \underset{\mathbf{x}}{\arg \min }\left\{\mathbf{c}^{\top} \mathbf{x} \mid \mathbf{A} \mathbf{x} \leq \mathbf{b}, \; \mathbf{x} \in \mathbb{Z}_+^{p} \times \mathbb{R}_+^{n-p}\right\},
\end{align}
where $ p $ is the number of integer variables, otherwise the variables are the same as \Cref{eq:lp}.

% https://texample.net/tikz/examples/colored-diagram/
\begin{figure}
    \centering
    \begin{tikzpicture}[
        thick,scale=0.5, 
        every node/.style={scale=0.2}
        every path/.style = {},
        every node/.append style = {font=\sffamily}
      ]
      \begin{scope}
        \shade[right color=gray, left color=white, opacity=0.7]
          (-0.5,-0.5) rectangle (0,6.5);
        \node[rotate=90, above] at (0,3) {};
        \shade[top color=gray, bottom color=white, opacity=0.7]
          (-0.5,-0.5) rectangle (8.5,0);
        \shade[left color=gray, bottom color=gray, right color=white, opacity=0.5]
          (-0.5,5.5) -- (8.5,3) -- (8.5,6.5) -- (-0.5,6.5) -- cycle;
        \path (-0.5,5.5) -- node[pos=0.23, sloped, above] {}
          (8.5,3);
        \shade[left color=gray, right color=white, opacity=0.5]
          (2.5,6.5) -- (8.5,6.5) -- (8.5,0) -- (5,0) -- cycle;
        \path (5,0) -- node[pos=0.3, sloped, above] {} (2.5,6.5);
        \node[text width=6em, align=center] at (2,2)
          {};
        \draw[->] (-0.5,0) -- (8.5,0) node[below] {x};
        \draw[->] (0,-0.5) -- (0,6.5) node[above] {y};
        \node[rotate=-45, above, text width=9em, align=center] at (7.25,5.25)
          {};
        \path[clip] (-0.5,-0.5) rectangle (8.5,6.5);
        \foreach \i in {0.5,3,...,13} {
          \draw[help lines] (-0.5,\i) -- +(-45:15);
        }
      \end{scope}
      \draw[very thick, ->] (9,3.25) -- node[above, text width=3cm, align=center]
        {} (11.5,3.25);
      \begin{scope}[shift={(13,0)}]
        \shade[right color=gray, left color=white, opacity=0.7]
          (-0.5,-0.5) rectangle (0,6.5);
        \shade[top color=gray, bottom color=white, opacity=0.7]
          (-0.5,-0.5) rectangle (8.5,0);
        \shade[left color=gray, bottom color=gray, right color=white, opacity=0.5]
          (-0.5,5.5) -- (8.5,3) -- (8.5,6.5) -- (-0.5,6.5) -- cycle;
       \shade[left color=gray, right color=gray, opacity=0.5]
         (2.5,6.5) -- (8.5,6.5) -- (8.5,0) -- (5,0) -- cycle;
        \draw[->] (-0.5,0) -- (8.5,0) node[below] {x};
        \draw[->] (0,-0.5) -- (0,6.5) node[above] {y};
        \foreach \i in {0,1,...,6.5} {
          \draw[help lines] (-0.5,\i) -- (8.5,\i);
        }
        \foreach \i in {2,4,...,8.5} {
          \draw[help lines] (\i,6.5) -- (\i,-0.5);
        }
        \foreach \i in {0,1,...,5} {
          \node[draw,cross out,label={left:\i}] at (0,\i) {};
        }
        \foreach \i in {0,1,...,4} {
          \node[draw,cross out] at (2,\i) {};
        }
        \foreach \i in {0,1,...,2} {
                \node[draw,cross out] at (4,\i) {};
        }
        \foreach \i in {0,2,...,6} {
          \node[below] at (\i,0) {\pgfmathparse{int(\i/2)}\pgfmathresult};
        }
      \end{scope}
    \end{tikzpicture}
    \caption{Illustration of an \Gls{LP} with its corresponding \Gls{ILP}, i.e. the \gls{LP} with integrality constraints.}
    \label{fig:milpfig}
\end{figure}

In \Cref{fig:milpfig}, an \gls{LP} problem and the problem with \textit{integrality constraints} is shown. For the \gls{LP} problem, the shaded areas represent the inequality constraints, where the diagonal lines represent the level curves of the objective function. In the \gls{ILP} problem, the crosses represent the feasible solutions. The \gls{LP} is also called a \textit{relaxation} of the original \gls{ILP}, which is fundamental to efficient solving algorithms of \gls{MILP} problems.

A problem that includes integrality constraints cannot be convex \cite{wolsey2020integer}. The non-convexity of the feasible set of the problem constitutes a significant challenge, and it is considered unlikely that polynomial-time solutions exist \cite{papadimitriou1982combinatorial}. \gls{MILP} problems belong to the category of $\mathcal{NP}$-hard problems \cite{papadimitriou1982combinatorial} (this class of problems will be discussed in \Cref{ssec:complexity}).   

A subset of \gls{MILP} problems can be integer linear programming (\gls{ILP}), where all variables are restricted to integer values, or binary linear programming (\gls{BLP}), where all variables are restricted to binary values. 

\gls{ILP} and \gls{BLP} problems belong to the category of \textit{combinatorial optimization} (\gls{CO}) problems, which has been the main focus of the efforts to solve entirely or partially with machine learning methods \cite{bengio2020machine}. 




\subsection{Computational Complexity}\label{ssec:complexity}

A basic understanding of computational complexity is required to justify the nature of the algorithms used to solve \gls{MILP} problems. 

Problems can be divided into \textit{classes} by the nature of the algorithms that can solve these problems. Problems for which there exist algorithms that can solve the problem in a time that is \textit{polynomial} of the problem size belong to class $\mathcal{P}$. Problems to which a correct solution can be verified in polynomial time belong to the class $\mathcal{NP}$ (non-deterministic polynomial time) \cite{cormen2009introduction}. 


Two other central complexity classes in this context are the $\mathcal{NP}$-complete and $\mathcal{NP}$-hard classes. The $\mathcal{NP}$-complete class contains problems that can be \textit{reduced} to any other problem in the $\mathcal{NP}$-complete class in polynomial time. 
The $\mathcal{NP}$-hard class contains problems that are at least as hard as the problems in the $\mathcal{NP}$-complete group but has not been proved to be reducible to a $\mathcal{NP}$-complete problem. An illustration showing this is given in \Cref{fig:np}. The general \gls{MILP} belongs to the $\mathcal{NP}$-hard class, and some \gls{MILP}s have been shown to belong to the $\mathcal{NP}$-complete class \cite{cormen2009introduction}. 


\begin{figure}
    \centering
    \includegraphics[width=0.4\linewidth]{img/npc.png}
    \caption{\label{fig:np}Illustration of the most commonly held view of the P, NP, NPC, and NP-hard relationship. Adapted from Cormen et al. (2009) \cite{cormen2009introduction}.}
\end{figure}

As stated, it is considered unlikely that \gls{MILP} problems can be solved in polynomial time. Therefore, it is more fruitful to improve upon the best existing solution algorithms and evaluating the improvements on practical problems. As the improvements attempted by substituting variable selection algorithms do not affect the running time complexity of the \gls{BnB} algorithm, this will not be discussed.  










\section{Branch and Bound}\label{sec:back_bnb}

A \textit{relaxation} of a \gls{MILP} problem is achieved by relaxing the integrality constraint, as shown in \Cref{fig:milpfig}. Obtaining the solution to the relaxed problem gives a lower bound on the optimal solution (for a minimization problem). Naturally, any feasible solution to the integrality-constrained problem gives an upper bound to the solution. Furthermore, if the solution to the relaxed problem adheres to the integrality constraints, it is also the solution to the \gls{MILP} problem \cite{wolsey2020integer}.






The most prevalent solution algorithm for \gls{MILP} problems exploits these results, by sequentially dividing the solution space until the optimum with the integrality constraint is found. This is done by branching in a binary tree structure according to \cite{gasse2019exact}:
\begin{align} \label{eq:branch}
    x_{i} \leq\left\lfloor x_{i}^{\star}\right\rfloor \vee x_{i} \geq\left\lceil x_{i}^{\star}\right\rceil, \quad \exists \; i \leq p \mid x_{i}^{\star} \notin \mathbb{Z}    
\end{align}
Further creating sub-problems with this binary decomposition. A general algorithm for this process is presented in \Cref{alg:bnb}, and an illustration of this process is shown in \Cref{fig:bandb1}.

\begin{algorithm}[H]
    \SetAlgoLined
    \KwResult{Optimal point and solution value of given problem.}
    Set $L = \{X\}$ and initialize $\hat{x}$\; 
    \While{$L \neq \emptyset $}{
        Select a subproblem $S$ from $L$ to explore\;
        \If{a solution $\hat{x}_* \in \{x \in S \;|\; f(x) < f(\hat{x})\}$ can be found}{
            Set $\hat{x} = \hat{x}_*$\;
        }
        \If{$S$ cannot be pruned}{
            Partition $S$ into $\{S_1, S_2 ..., S_r\}$\;
            Insert $\{S_1, S_2 ..., S_r\}$ into $L$\;
        }
        Remove  $S$ from $L$\;
    }
    Return $\hat{x}$\;
    \caption{\label{alg:bnb} A generic branch-and-bound algorithm \cite{morrison2016branch}.}
\end{algorithm}

%https://tex.stackexchange.com/questions/416359/branch-and-bound-tree-in-tikz
\comments{
\begin{figure}
\centering
\begin{forest}
  branch and bound,
  where level=1{
    set branch labels={x\leq}{}{x\geq}{},
  }{
    if level=2{
      set branch labels={}{\geq y}{}{\leq y},
    }{},
  }
  [1055.56:S:950
    [1000:S_1:950:5
    ]
    [1033:S_2:950:6
      [1033:{S_2,1}:950:1]
      [950:{S_2,2}:1033:2]
    ]
  ]
\end{forest}
\caption{\label{fig:bandb1}Illustration of the branch and bound algorithm}
\end{figure}}

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{img/bnb.png}
    \caption{\label{fig:bandb1}Illustration of the branch-and-bound algorithm adapted from a maximization problem in \textit{Integer Programming }(2020) \cite{wolsey2020integer}.}
\end{figure}

For each generated solution, represented by nodes in \Cref{fig:bandb1}, a relaxation of the problem is solved in order to obtain an upper and lower bound on the solution of the sub-problem. 
These values are shown on the top and bottom right, respectively. 
Generating upper and lower bounds for solutions allows for discarding a large number of solutions \cite{wolsey2020integer}. Branches can be \textit{pruned} (meaning no further partitioning from that branch) if they meet at least one of the following three criteria \cite{wolsey2020integer}:
\newpage
\begin{enumerate}[label=(\roman*)]
    \item Pruning by optimality: $Z^t = \{\max \bm{c}^{\top} \bm{x} : \bm{x} \in S_t\}$ has been solved.
    \item Pruning by bound: $\overline{Z}^t \leq \underline{Z}^t$.
    \item Pruning by infeasiblity: $S_t = \emptyset $.
\end{enumerate}
For \Cref{fig:bandb1}, the graph on the right represents the tree after solving the relaxation, resulting in $S_2$ being pruned by infeasibility, $S_3$ pruned by bound, and $S_4$ pruned by optimality.


The choice of node and variable to branch on to find the optimum in the fewest number of branching processes is central to an efficient implementation of \gls{BnB}. Partitioning the feasible set such that the node with the optimal value is found in the fewest possible branching iterations is the optimal policy. 


\subsection{Valid Inequalities}\label{ssec:inequalities}

Another important method used in \gls{BnB} algorithms is the concept of valid inequalities. A valid inequality is an inequality that does not remove feasible solutions of the non-convex solution set but can remove potential solutions to the relaxed problems. A valid inequality can be expressed as:
\begin{equation}\label{eq:cut}
    \bm{\pi}^{\top} \mathbf{x} \leq \pi_0 \quad \forall \; \mathbf{x} \in  \bm{X}   
\end{equation}
where $\bm{X}$ is the feasible set as described in \Cref{eq:milp}. These inequalities reduce the size of the feasible set for the relaxations of the problem without removing feasible solutions of the original problem. An illustration of an \gls{ILP} with an added valid inequality is shown in \Cref{fig:cut}. Here the feasible set of the relaxation is reduced in size by the added constraint, while the feasible points of the \gls{ILP} remain feasible after the application of the inequality, as is given in \Cref{eq:cut}.

\begin{figure}
    \centering
    \begin{tikzpicture}[
        thick,scale=0.5, 
        every node/.style={scale=0.2}
        every path/.style = {},
        every node/.append style = {font=\sffamily}
      ]
      \begin{scope}
        \shade[right color=gray, left color=white, opacity=0.7]
          (-0.5,-0.5) rectangle (0,6.5);
        \shade[top color=gray, bottom color=white, opacity=0.7]
          (-0.5,-0.5) rectangle (8.5,0);
        \shade[left color=gray, bottom color=gray, right color=white, opacity=0.5]
          (-0.5,5.5) -- (8.5,3) -- (8.5,6.5) -- (-0.5,6.5) -- cycle;
       \shade[left color=gray, right color=gray, opacity=0.5]
         (2.5,6.5) -- (8.5,6.5) -- (8.5,0) -- (5,0) -- cycle;
        \draw[->] (-0.5,0) -- (8.5,0) node[below] {x};
        \draw[->] (0,-0.5) -- (0,6.5) node[above] {y};
        \foreach \i in {0,1,...,6.5} {
          \draw[help lines] (-0.5,\i) -- (8.5,\i);
        }
        \foreach \i in {2,4,...,8.5} {
          \draw[help lines] (\i,6.5) -- (\i,-0.5);
        }
        \foreach \i in {0,1,...,5} {
          \node[draw,cross out] at (0,\i) {};
        }
        \foreach \i in {0,1,...,4} {
          \node[draw,cross out] at (2,\i) {};
        }
        \foreach \i in {0,1,...,2} {
                \node[draw,cross out] at (4,\i) {};
        }
      \end{scope}
      \draw[very thick, ->] (9,3.25) -- node[above, text width=3cm, align=center]
        {} (11.5,3.25);
      \begin{scope}[shift={(13,0)}]
        \shade[right color=gray, left color=white, opacity=0.7]
          (-0.5,-0.5) rectangle (0,6.5);
        \shade[top color=gray, bottom color=white, opacity=0.7]
          (-0.5,-0.5) rectangle (8.5,0);
        \shade[left color=gray, bottom color=gray, right color=white, opacity=0.5]
          (-0.5,5.5) -- (8.5,3) -- (8.5,6.5) -- (-0.5,6.5) -- cycle;
        \shade[left color=gray, right color=gray, opacity=0.5]
          (2.5,6.5) -- (8.5,6.5) -- (8.5,0) -- (5,0) -- cycle;
        \shade[left color=gray, right color=gray, opacity=0.5]
          (0.0,6.5) -- (8.5,6.5) -- (8.5,0) -- (6.0,0) -- cycle;
        \draw[->] (-0.5,0) -- (8.5,0) node[below] {x};
        \draw[->] (0,-0.5) -- (0,6.5) node[above] {y};
        \foreach \i in {0,1,...,6.5} {
          \draw[help lines] (-0.5,\i) -- (8.5,\i);
        }
        \foreach \i in {2,4,...,8.5} {
          \draw[help lines] (\i,6.5) -- (\i,-0.5);
        }
        \foreach \i in {0,1,...,5} {
          \node[draw,cross out] at (0,\i) {};
        }
        \foreach \i in {0,1,...,4} {
          \node[draw,cross out] at (2,\i) {};
        }
        \foreach \i in {0,1,...,2} {
                \node[draw,cross out] at (4,\i) {};
        }
      \end{scope}
    \end{tikzpicture}
    \caption{Figure of an \Gls{ILP} before and after an added valid inequality.}
    \label{fig:cut}
\end{figure}
 


Algorithms that find these inequalities during the \gls{BnB} algorithm are called \textit{branch-and-cut}. The nomenclature comes from calling the application of these inequalities \textit{cuts} or \textit{cutting planes}.
When an application of inequalities are only employed on the root node (before dividing the solution space in the enumeration), the algorithm is sometimes referred to as \textit{cut-and-branch} rather than \textit{branch-and-cut} \cite{wolsey2020integer}.






\subsection{Primal and Dual Heuristics}

The modern implementations of \gls{BnB} solvers base their efficiency on the implementation of \textit{heuristics} \cite{khalil2020towards}, which are divided into the classes \textit{primal} and \textit{dual}. Heuristic is synonymous with "human-designed rule" in this context.

\textit{Primal heuristics} are methods for finding feasible solutions at a given \gls{BnB} node, where the quality, i.e. the distance to the optimal bound, is the determining factor to whether the feasible solution is useful or not \cite{khalil2020towards}. These heuristics are as costly as they are useful, and modern solvers periodically run different heuristics at different times during the solution process \cite{khalil2020towards}.

\textit{Dual heuristics} are the methods that find the lower bound of the optimization problem. This includes solution of relaxations of the problem as well as the addition of valid inequalities. 

The relationship is summarized in this quote from Khalil (2020) \cite{khalil2020towards}:
\begin{quote}
    [...] the
primal side refers to the quest for good feasible solutions, whereas the dual side refers to
the search for a proof of optimality.
\end{quote}






\subsection{Branching Variable Selection Policy}\label{ssec:branchingpolicy}

As mentioned, an important decision in the \gls{BnB} algorithm is the choice of the variable that should be branched on. 
There exists many heuristics for solving this, who vary in computational complexity and accuracy. 
A good branching algorithm should choose to branch on variables that lead to small solution trees (fewer nodes evaluated) and find these variables in a computationally efficient manner. 

All popular variable selection policies depend on scoring the candidate branching variables, expressed as $s_i \in \mathbb{R}^1 \; \forall \: i \in \mathcal{C}$, and then selecting the variable with the most optimal score \cite{achterberg2004branching}. The branching operation generates two child nodes, $Q_i^-$ and $Q_i^+$. The branching candidate comparison is done by comparing the two objective function changes of each candidate, denoted as $\Delta_i^- \coloneqq \bar{c}_{Q_i^-}-\bar{c}_{Q}$ and $\Delta_i^+ \coloneqq \bar{c}_{Q_i^+}-\bar{c}_{Q}$ \cite{achterberg2004branching}. The final score is then typically calculated by a function similar to \cite{achterberg2004branching}:
\begin{equation}
    score(q^- , q^+ ) = (1 - \mu) \cdot \min \{q^- , q^+ \} + \mu \cdot \max \{q^- , q^+ \}\:,\quad \mu \in \left[0, 1\right]    
\end{equation}



The current branching policy resulting in the smallest solution trees is known as \textit{strong branching} (\gls{SB}) \cite{applegate1995finding}, and the application of this branching policy at every node is known as \textit{full strong branching} (\gls{FSB}) \cite{achterberg2004branching}. This branching policy is based on determining the best variable to branch on by solving the relaxation for every candidate variable, and is therefore very computationally expensive compared to other methods \cite{achterberg2004branching}.

Another branching policy is \textit{most infeasible branching} (\gls{MIB}), where the variable with the fractional part of the relaxation optimum closest to $0.5$ is selected. This policy, though computationally inexpensive, has proved to be very poor \cite{achterberg2004branching}. 
 
An effective and popular policy is \textit{pseudo-cost branching} (\gls{PC}), which relies on the expected change in objective value based on previous branching on the variable in question
\cite{achterberg2004branching}. In short, the objective gain per unit change in a variable is averaged over all nodes where it has been branched upon. This value is termed the \textit{pseudo-cost} of the variable. As is evident, these values depend on a history of branching, and will therefore be inaccurate for the first decisions \cite{achterberg2004branching}. 

The policy known as \textit{reliability pseudo-cost branching} (\gls{RPC}) aims to mitigate the inaccuracy of the \gls{PC} algorithm by combining \gls{SB} and \gls{PC} \cite{anand2017comparative}. In \gls{RPC}, \gls{SB} is employed for variables that are either uninitialized (never branched on before) or have \textit{unreliable} pseudo-costs (pseudo-costs that stem from little data) \cite{achterberg2004branching}. This policy is the standard of the \gls{SCIP} optimization suite \cite{achterberg2009scip}. 

In the literature, the branching policy is referred to as a policy, strategy, or rule. In this thesis \textit{policy} is used.  








\subsection{Learned Branching Policy}

Recently, attempts have been made to find a branching policy based on statistical learning. 

Using machine learning, specifically imitation learning, to find good candidate variables for branching in a less computationally demanding manner was proposed by Elias Khalil \cite{khalil2016learning}. Various methods for learning in branching include \textit{ support vector machine ranking} (\Gls{SVM}) \cite{khalil2016learning}, \textit{graph convolutional neural networks} (\gls{GCNN}) \cite{gasse2019exact} and \textit{feature-wise linear modulation} (\gls{FiLM}) \cite{gupta2020hybrid}.

The fundamental assumption to this approach is that a computationally efficient approximation to the most computationally demanding but most accurate branching policy can be learned. The algorithm will use imitation learning on the branching expert to find a computationally less expensive non-linear function approximation to the expert algorithm's variable scoring. Then, the algorithm branches on the variable with the highest score. 
%This can be expressed as: 
%\begin{align}
%    f(i) &=  \pi_{SB} (i) + \epsilon \quad \forall \; i \in \mathcal{C}\\
%    f(i) &= s_i\\
%    i^*_f &= \underset{i \in \mathcal{C}}{\mathrm{argmin}} \; \bm{s}_i
%\end{align}
%where $f$ is the learned function, $\mathcal{C}$ is the set of possible branching variables, $\pi_{SB}$ is the Strong Branching strategy and $\epsilon$ is the deviation in the scoring function. 




\section{Markov Decision Processes}\label{sec:back_mdp}

This section presents the \textit{Markov decision process} formulation of the variable selection problem. 


\subsection{Markov Decision Processes Formulation}\label{ssec:mdp}

Central to the advancement of learned policies in \gls{BnB} is the interpretation of the solution algorithm as an \textit{agent} in a \textit{Markov decision process} (\gls{MDP}) \cite{gasse2019exact}. This interpretation relates the problem to a large collection of literature on the topic \cite{howard1960dynamic}.

In an \gls{MDP}, the agent is at time $t$ in a state $\mathcal{S}_t$, from which it performs an action $\mathcal{A}_t$ that transforms the agent to the state $\mathcal{S}_{t+1}$ and receives the \textit{reward} $\mathcal{R}_{t+1}$ \cite{prouvost2021ecole}. The probability of an agent performing action $a$ in state $s$ is given as $\pi (a | s)$. The probability distribution for the agent to transition to a new state $s'$ is given as $\mathbb{P}(s', r | a, s)$ \cite{prouvost2021ecole}. 

 
A sequence of actions generates a sequence of trajectories $\tau$, and is described as an \textit{episode}. The probability of a trajectory is given in Prouvost et al. (2021) \cite{prouvost2020ecole} as:
\begin{equation}
    \mathbb{P}(\tau) \sim \underbrace{\mathbb{P}(\mathcal{S}_0)}_{\text{initial state}}
\prod_{t=0}^\infty \underbrace{\pi(\mathcal{A}_t | \mathcal{S}_t)}_{\text{next action}}
\underbrace{\mathbb{P}(\mathcal{S}_{t+1}, \mathcal{R}_{t+1} | \mathcal{A}_t, \mathcal{S}_t)}_{\text{next state}}
\end{equation}

These definitions now allow a formulation of the \gls{MDP} control problem, which is the problem of interest in this thesis. The control problem consists of finding the action policy that maximizes the reward. %, and can be stated as: 
%\cite{prouvost2020ecole}
%\begin{equation}\label{eq:mdprcontrol}
%    \pi^\star = \underset{\pi}{\operatorname{arg\,max}}
%\lim_{T \to \infty} \mathbb{E}_\tau\left[\sum_{t=0}^{T} %\mathcal{R}(\mathcal{S}_t)\right]
%\end{equation}



\subsection{Partially-observable Markov Decision Processes}

A subset or generalization of an \gls{MDP} is the \textit{partially-observable Markov decision process }(\gls{PO-MDP}) \cite{monahan1982state}. Processes of this class allow for uncertainty of the states as well as additional acquisition of state information \cite{monahan1982state}. The agent will therefore decide actions based on the observation of the state, given as $\mathcal{O}$ \cite{prouvost2021ecole}. All past observations of the observations, rewards and actions are given in the history $\mathcal{H}_t$, given as \cite{prouvost2021ecole}:
\begin{equation}
    \mathcal{H}_t = \{\mathcal{O}(\mathcal{S}_0), \mathcal{R}(S_0), \mathcal{A}_0, ..., \mathcal{O}(\mathcal{S}_{t-1}), \mathcal{R}(S_{t-1}), \mathcal{A}_{t-1}, \mathcal{O}(\mathcal{S}_t)\}
\end{equation}
The generalization from \gls{MDP} to \gls{PO-MDP} concedes the Markovian nature of the trajectories \cite{prouvost2020ecole}.

In addition, the initial state is given by the distribution of the problem instance $I$, giving the relation $\mathbb{P}(\mathcal{S}_0) = \mathbb{P}(I) \mathbb{P}(\mathcal{S}_0 | I)  $ \cite{prouvost2021ecole}. 

This results in the final formulation \cite{prouvost2021ecole}:
\begin{equation}
    \mathbb{P}(\tau) \sim \underbrace{ \mathbb{P}(I) \mathbb{P}(\mathcal{S}_0 | I)  }_{\text{initial state}}
\prod_{t=0}^\infty \underbrace{\pi(\mathcal{A}_t | \mathcal{H}_t)}_{\text{next action}}
\underbrace{\mathbb{P}(\mathcal{S}_{t+1}, \mathcal{R}_{t+1} | \mathcal{A}_t, \mathcal{S}_t)}_{\text{next state}}
\end{equation}

An illustration of the Markov decision process control loop from the documentation of \gls{Ecole} is shown in \Cref{fig:mdp}.

\begin{figure}
    \centering
    \includegraphics[width=0.55\linewidth]{img/mdp.png}
    \caption{Illustration of the Markov decision process control loop. Figure from Prouvost et al. (2021) \cite{prouvost2021ecole}.}
    \label{fig:mdp}
\end{figure}



\subsection{Branch \& Bound as a PO-MDP}\label{ssec:pomdp}

Interpreted in the language of \gls{MDP}s, the \gls{BnB} algorithm is the \textit{environment} and a concrete \gls{MILP} problem instance is an \textit{episode} in this environment. The \textit{agent} is the brancher, where in this thesis the variable selection policy is the component of interest, ignoring the node selection policy. The state of the solver consists of the \gls{BnB} tree at that instance, as well as the observations at each node (the \textit{history} of the \gls{PO-MDP}).

This formulation is the basis for the \textit{Ecole} framework, which is discussed in \Cref{ssec:ecole}
The \gls{PO-MDP} formulation allows for the agent in the \gls{BnB} environment to be learned through reinforcement learning, discussed in \Cref{ssec:back_rl}.



\subsection{Branch \& Bound Observation}\label{ssec:obs}

A prerequisite for learning in \gls{BnB} is the observation of the state of the episode, i.e. the state of the solver of an instance at a specific node in the solution tree. 

%Little attention towards these features are found in the major publications in this field (\cite{gasse2019exact,gupta2020hybrid}), however the observation is the foundation of the learning process.
%\textit{Learning to Branch} by Khalil et al. (2016) \cite{khalil2016learning} contains multiple additional features, however these will not be discussed in this thesis. 

The features of a \gls{BnB} node are divided into three classes: variable features, constraint features and edge features.


\textbf{Variable Features}

For a candidate branching variable, relevant features include the type of the variable (binary, integer, etc.), whether the variable has a defined lower and/or upper bound, and whether the solution is at at either of these bounds.  
If not, the variable has a fractionality that represents the solution of the relaxed problem.
At the solution node, the incumbent has a value that can be compared to incumbents at other nodes, as well as the relative impact of the variable on the objective value in the incumbent. 
The variable also has a state with respect to the solution of the relaxation with a simplex solution algorithm --- if the variable is a basic or non-basic variable or other information relating to this solution.
Presented in Khalil et al. (2016) \cite{khalil2016learning} are also a number of other features that will not be utilized in this work.




\textbf{Constraint Features}

The cosine similarity represents a coefficient of the angle between the variable and the constraint.   
The bias of the constraint is also included. 
An additional feature is whether the variable is at the constraint in the relaxation
Each constraint also has a value from the solution of the dual problem. 


\textbf{Edge Features}

The edge features consist of the constraint coefficient, meaning the coefficient that is multiplied with the candidate variable. This will also give the relations between constraints and variables.




\subsection{Bipartite Graph Representation}

The application of \gls{GCNN}s on \gls{MILP} and sub-\gls{MILP} problems rely on the bipartite representation of constraints and variables as presented in Gasse et al. (2019) \cite{gasse2019exact}.
This concept will be introduced with an example \gls{MILP} given as:
\begin{align}\label{eq:bipex}
    \min \quad &\texttt{v}_1 + \texttt{v}_2 + \texttt{v}_3\\ 
    s.t. \quad &\texttt{v}_1 + \texttt{v}_2 - \texttt{v}_3 \geq 1 \qquad (c_1)\nonumber\\
    &\texttt{v}_3 \geq \frac{1}{2}\qquad\qquad\quad\;\,\; (c_2)\nonumber\\
    &\mathbf{v} \in \mathbb{B}^3 \nonumber
\end{align}

\begin{figure}
    \centering
    \includegraphics[width=0.40\linewidth]{img/bipartite_zoom.png}
    \caption{Example of a bipartite constraint-variable graph.}
    \label{fig:bipartite_cv}
\end{figure}

For \Cref{eq:bipex}, the corresponding constraint-variable graph representation can be illustrated as in
\Cref{fig:bipartite_cv}.

Constraints and variables are the numbered nodes of the graphs, while the edges represent the relation between the nodes. 



\section{Machine Learning Models}\label{sec:back_models}


The \gls{ML}-models used in the thesis are presented in this section. First the multi-layer perceptron and graph convolutional neural network models, then the concepts of \textit{ablation studies} and \textit{reinforcement learning}.

\subsection{Multi-layer Perceptrons}\label{ssec:back_mlp}

Multi-layer perceptrons (\gls{MLP}s), more commonly known as deep feed-forward neural networks, are recommended by Gupta et al. (2020) \cite{gupta2020hybrid} as a less computationally expensive alternative to the approaches by Khalil et al. (2016) \cite{khalil2016learning} and Gasse et al. (2019) \cite{gasse2019exact}. 

\gls{MLP}s are networks that generate a nonlinear function $y = f(\mathbf{x}; \bm{\theta})$, where $x$ is the input, $y$ is the output, and $\bm{\theta}$ represents the parameters of the function. The parameters are learned during repeated optimization, and will under ideal circumstances converge to approach the optimal function $y = f^*(\mathbf{x})$. The function is realized as a series of compositions of functions. The composed functions are represented as an acyclical, directed graph \cite{nielsen2018neural}, and can be expressed as:
\begin{align}
    y = f_L \circ f_{L-1} \circ \ldots \circ f_{1} \circ f_{0} (\mathbf{x})  
\end{align}
The functions are denoted as \textit{layers} of the perceptron, and are implemented as affine functions of every input parameter at every node, $\mathbf{z}_l = \mathbf{x}_{l-1}^T \mathbf{w}_l + \mathbf{b}_l$. Applying non-linear function, known as an \textit{activation function}, allows the \gls{MLP} to represent arbitrary nonlinear functions \cite{goodfellow2016deep}. This is expressed as $\mathbf{x}_l = \mathbf{a}(\mathbf{z}_l)$.

The computation of the output of the function given its input is known as a \textit{forward pass} through the network. The required computations for a single input vector into a network with $ n $ hidden layers will include $ n + 1 $ matrix multiplications and $ n + 1 $ applications of the non-linear activation function, given that there is an activation function on the output. 

%Depending on the training configuration, the problem can be interpreted as a classification problem or regression problem (or a ranking problem, as in \cite{khalil2016learning}). In the following experiments, the former approach is selected, as has become popular after Gasse et al. (2019) \cite{gasse2019exact}. 










\subsection{Graph Convolutional Neural Networks }

Graph convolutional neural network (\gls{GCNN}) is a term for neural networks that have input data represented in a graph-structure that is processed by a convolution operation \cite{kipf2016semisupervised}. In this thesis, the terms \gls{GCNN} and \gls{GNN} will be used interchangeably. 

The fundamental property of the graph convolution is its ability to create representations of irregular data without altering the structure of the data. This means that, for instance, nodes that share vertices can pass information to each other, so the feature representation of a node can utilize the features of neighboring nodes.  
An illustration of this using the bipartite graph from \Cref{fig:bipartite_cv} is given in \Cref{fig:conv_ex}. The features of the nodes are transformed while maintaining the structure of the graph.

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{img/conv_example.png}
    \caption{Example of a graph convolution on a bipartite constraint-variable graph.}
    \label{fig:conv_ex}
\end{figure}

A graph convolution can be expressed as a matrix/tensor multiplication followed by a nonlinear activation function, as in \Cref{sec:back_mdp}. This will be explained in the case of a undirected graph for the sake of simplicity. A prerequisite for this is the representation of the graph with the adjacency matrix, denoted as $\Tilde{\mathbf{A}}$. The adjacency matrix is a square $|\mathbf{V}|\times|\mathbf{V}|$ matrix containing 0 or 1 depending on whether the pair of vertices are connected or not \cite{kipf2016semisupervised}. In addition the adjacency matrix, the degree matrix $\Tilde{\mathbf{D}} = \sum_j \Tilde{\mathbf{A}}_{i j}$ is necessary in order to normalize the operation \cite{kipf2016semisupervised}.

With the given definitions, the graph convolution operation can be expressed by the propagation rule \cite{kipf2016semisupervised}:
\begin{equation}
    \mathbf{H}^{(l+1)} = a \left( \Tilde{\mathbf{D}}^{-\frac{1}{2}} \Tilde{\mathbf{A}} \Tilde{\mathbf{D}}^{-\frac{1}{2}} \mathbf{H}^{(l)}\mathbf{W}^{(l)}\right)
\end{equation}
where $\mathbf{H}^{(l)}$ is the matrix of activations in layer $l$, with the first layer $\mathbf{H}^{0}=\mathbf{X}$.  $\mathbf{W}^{(l)}$ is a layer of learned weights. $a( \cdot) $ is a nonlinear activation function.
This operation is inspired by the first-order approximations to spectral filters on graphs \cite{kipf2016semisupervised}.





Models that leverage the graph nature of combinatorial optimization problems have been shown to have satisfactory performance, see e.g. Dai et al. (2018) \cite{dai2018learning}. 
\gls{GCNN}s are proposed by Gasse et al. (2019) \cite{gasse2019exact} as an alternative to the feature-rich approaches by Khalil et al. (2016) \cite{khalil2016learning}. 
The application of \gls{GCNN}s on \gls{BnB} algorithms rely on the bipartite constraint-variable representation at each node of the \gls{BnB} solution tree.


The term \textit{embeddings} will be used in this thesis for continuous-variable representations derived from the input features, as it is used in Gasse et al. (2019) \cite{gasse2019exact}. 


The state of the \gls{BnB} graph at a node can be represented as $s_t = (\mathcal{G}, \mathbf{C}, \mathbf{E}, \mathbf{V})$, where $\mathcal{G}$ represents the bipartite \gls{BnB} solution graph at that time instance, $\mathbf{C}$ represents the constraints, $\mathbf{E}$ represents the \textit{edges} (connections) between the variables and constraints, and $\mathbf{V}$ represents candidate variables.  

Gasse et al. (2019) \cite{gasse2019exact} presents three motivating points for why graph convolutions would be a good architecture for learning to branch:
\begin{enumerate}[label=(\roman*)]
    \item They are well-defined no matter the input graph size.
    \item Their computational complexity is directly related
to the density of the graph, which makes it an ideal choice for processing typically sparse \gls{MILP}
problems.
    \item They are permutation-invariant, that is they will always produce the same output no
matter the order in which the nodes are presented.
\end{enumerate}






\subsection{Ablation Studies}

The concept of ablation studies in machine learning, as presented in 
Meyes et al. (2019) \cite{meyes2019ablation} is presented in this section.
Ablation studies hail from the field of neuroscience, in which a complex system, e.g. the brain, is examined after removing different sections. The function of the removed sections can then be inferred by the change in the observed reaction to external stimuli \cite{meyes2019ablation}.

In the context of \gls{ML}, ablation studies are a formalization of observing changes in performance after the removal of components of artificial neural networks \cite{meyes2019ablation}.  
The concept, or at least the formalization, is not yet considered a standard method in \gls{ML} research \cite{sheikholeslami2019ablation}.
In this thesis, the concept of an ablation study will be interpreted more broadly than in Meyes et al. (2019) \cite{meyes2019ablation}, as the networks in this thesis are retrained after each section is removed. This form of ablation study is coined as \textit{model ablation} in Sheikholeslami (2019) \cite{sheikholeslami2019ablation}.





\subsection{Reinforcement Learning}\label{ssec:back_rl}

The subset of machine learning described as \textit{reinforcement learning} (\gls{RL}) is highly relevant in the context of \gls{ML} in \gls{CO}. No results will be discussed in this thesis, however, a background in the topic is necessary to understand both related work and the long-term goals of the field.

\gls{RL} encompasses the problem of an \textit{agent} learning a \textit{policy} for behaving in an \textit{environment} so as to achieve a global objective. Any sequential decision-making problem with a measure of optimality that relies on past experience can be formulated as a \gls{RL} problem \cite{francois2018introduction}. The approach has seen success in a number of fields in the past years with the integration of deep learning models, often termed deep \gls{RL} \cite{francois2018introduction}. Most notable of the advancements might be AlphaZero, Google's successful chess-AI \cite{silver2017mastering}. \gls{RL} has the important property of being independent of data, meaning a number of core \gls{ML} challenges (quantity, quality, and bias of data) are rendered irrelevant \cite{goodfellow2016deep}. 

%\gls{RL} is particularly interesting in \gls{BnB} because of the \gls{MDP} nature of the algorithm, and the assumption that the handcrafted heuristics and sub-algorithms prevalent in modern solvers are either inefficient or inaccurate compared to the theoretical capabilities of, i.e. neural networks. 

Many attempts have been made at implementing \gls{RL} in \gls{CO}, see for example Etheve et al. (2020) \cite{etheve2020reinforcement} or Tang et al. (2020) \cite{tang2020reinforcement}. Approaches for learning variable selection, such as reported in Scavuzzo (2020) \cite{scavuzzo2020learning}, rely on efficient and accurate pre-trained models based on imitation learning, like the models presented in this thesis. More knowledge is likely needed for the pure \gls{RL} approach to take over the mantle. Recently,
Cappart et al. (2021) \cite{cappart2021combinatorial} also concluded that useful \gls{RL} policies are not mature yet. 
