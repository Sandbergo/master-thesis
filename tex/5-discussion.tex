\chapter{Discussion}\label{cha:discussion}

This chapter discusses the results obtained in \Cref{cha:results} and presents ideas about further work in the field.


\section{Problem Instances}

Problem instances were only created from the class of combinatorial auctions. From the training results, it can be assumed that generating $10000$ problems is not necessary, as the model converges before all problems are seen.  

Increasing the problem size was chosen as the method for estimating the out-of-distribution efficiency of the \gls{MLP}-aided \gls{BnB}, as was done by Gupta et al. \cite{gupta2020hybrid}. Other approaches to the problem generation and generalization efficiency estimation might look into problem distributions with a temporal component, i.e. where the test samples are from a provably different distribution, without dramatically increasing the difficulty. This might be more representative of problems that are time-constrained, and will give a different estimate of the out-of-distribution generalization error.  


\section{Accuracy}

The accuracy is on par with the computationally heavy (or \gls{GPU}-dependent) Graph Convolutional Neural Network approach, with around a $3 \%$ higher accuracy than the \gls{MLP}-methods presented in \Cref{fig:topk}. MLP1 had a minor reduction in accuracy compared to MLP2 and MLP3, which had nearly indistinguishable results.   

The near-equal accuracy of the single-layer perceptron and the multi-layer perceptrons is assumed to have two possible explanations: 
\begin{enumerate}[label=(\roman*)]
    \item The training protocol is not able to discover the underlying nonlinear relation of the features to the optimal branching function. 
    \item The theoretical optimal branching function is not strongly dependent on non-linear interactions of the available features.
\end{enumerate}
The latter argument seems more likely based on the results in this project and previous experiments \cite{gupta2020hybrid} \cite{gasse2019exact}, however there are no convergence guarantees for the MLPs, and option one can therefore not be ruled out. Further analysis of the resultant linear function of MLP1 might prove further insights into this.

The fast convergence close to the optimum showed by the training graphs might indicate that a near-optimal function approximation can be found through a small amount of training data. This is consistent with the results for the three MLPs, however, this analysis was not found in Gupta et al. \cite{gupta2020hybrid}.

The practically insignificant accuracy of MLP1 and MLP2 has similar arguments. This indicates that increasing the capacity of the model is not a productive strategy for learning to branch with the current feature set. 

\section{Efficiency}

The central problem in \textit{learning-to-branch} is the trade-off between computational efficiency and the number of nodes processed. The results in \Cref{tab:results1_cauction} show that an increase in complexity can very negatively impact the solution time. MLP3 has a considerably longer solution time than MLP2, even though the average  number of evaluated nodes is only 1 \% more for MLP3. This indicates a large amount of the solution time is spent in the forward pass of the MLP. MLP1 gains a 17 \% decrease in computation time at the cost of 8 \% more nodes evaluated. The \gls{MLP} with optimal efficiency can be a higher capacity network than MLP1, however, the reduced implementation complexity of the linear model might make up for this discrepancy in accuracy. 

%Gupta et al. \cite{gupta2020hybrid} recommends the \gls{FiLM} model for an efficient forward pass, however, the results in this project show that linear models can give close to the same accuracy, and would therefore be preferred over more complex models when the accuracy discrepancy remains small.  


\section{Further Work}

The time-consuming process of further verification of the \gls{MLP} models on more problem sets and averaging over random seeds as in Gupta et al. \cite{gupta2020hybrid} remains to be done to increase the certainty of the findings. After this, a comprehensive cross-check of the results in the two studies might lead to valuable insights into the differences in the approaches.   

Further work in the field should also strive to compare results with the top commercial solvers IBM CPLEX and Gurobi \cite{anand2017comparative}. The automatic tuning of the highly parameterized commercial optimization solvers has also been shown to yield significant improvements in solution time \cite{hutter2010automated}, and is therefore advised to include in further and more comprehensive comparisons.  

The results in this project show that for the features of the branching variables, a linear classifier might suffice to outperform Reliability Pseudo-cost Branching as compared to more demanding methods. It seems reasonable to assume that there can be a significant increase in efficiency with an implementation of the linear branching rule, either directly into \gls{SCIP} or via a more performance-conscious interface with PySCIPOpt. The assumed improvement in efficiency might further encourage work on learned function approximations as components of solution algorithms. Another opportunity with linear classifiers showing close-to-optimal accuracy with Deep Neural Networks is the increase in methods for parameter analysis from the resulting optimal model. Analyzing this can yield useful insights into the core components of the successful functions, which appears unexplored in the literature.

The work in this field remains on artificial problems, however, there would be great interest in a practical implementation on real-world optimization problems, e.g. an optimal traffic routing algorithm running on an embedded system. 

What appears unexplored by the community working on supervised learning in \gls{BnB} is the regression problem of predicting the Strong Branching score, as this is an unused feature. The correlation of the variable features and the objective function improvement might further provide insights into the possibilities and limitations of function approximations to variable selection algorithms. 

A recurring approach mentioned in the context of improving \gls{BnB} is the Reinforcement Learning approach. This appears to be under development, with results in review by Etheve et al. \cite{etheve2020reinforcement} and development of a training suite by Prouvost et al. \cite{prouvost2020ecole}. On a larger scale, the work on learning-to-branch fits in the larger context of learned algorithms replacing expert-created algorithms. This has the potential to eventually remove the human component to algorithm construction. For now, closer attention to the features and correlations of the variable scoring might be the more fruitful endeavor, as the results in this project show that the deep learning approach does not yield significantly better results. 


\section{Research Questions}

Now, we return to the research questions presented in \Cref{sec:questions}:
\begin{enumerate}[label=(\roman*)]
    \item \textit{Can Multi-Layer Perceptrons decrease the running time of Branch and Bound running on the \Gls{CPU}?}
\end{enumerate}
Based on the results presented in \Cref{cha:results}, the conclusion is \textit{yes} when compared to the Full Strong Branching, Pseudo-cost Branching and Reliability Pseudo-cost Branching strategies found in the SCIP Optimization solver. 
\begin{enumerate}[resume*]
    \item \textit{What is the impact of different \gls{MLP} model sizes on the accuracy and efficiency of the Branch and Bound algorithm}?
\end{enumerate}
Both accuracy and efficiency was found to vary over the different \gls{MLP} design choices. This implies that larger models, at least up to a point, would see higher accuracy in branching variable choice. However, increased model complexity gives slower inference time, showing that for the conducted experiments, the best performing model was a linear model.
\begin{enumerate}[resume*]
    \item \textit{Are there more research opportunities for \gls{MLP}s in Branch and Bound?}
\end{enumerate}
Positive results indicate many opportunities in this research field. Firstly, further verifying the results in this project on other problems and configurations is needed for a larger degree of certainty on their usefulness. Further, the analysis of the \gls{MLP}s might give the insights mentioned in \cite{lodi2017learning}, that are still unanswered, particularly the single-layer model is a good candidate for this.   