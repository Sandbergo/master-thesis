%!TEX root = ../Thesis.tex
\chapter{Introduction}\label{cha:introduction}
%
This chapter presents a short history, motivation and background in the fields of mathematical programming and machine learning, summarizes the previous work on the topic of \textit{learning-to-branch}, poses the research questions for the project, and explains the structure of the project.


\section{Background}

In this section, a relevant background in the relevant fields of mathematical programming and machine learning is presented, as well as an overview of the previous work in combining these fields. The section is adapted from the project report \textit{Multi-layer Perceptrons for Branching in Mixed-Integer Linear Programming }(2020). 

\subsection{Mathematical Programming}

The invention of efficient solution algorithms to linear mathematical programming problems is considered one of the great post-war inventions \cite{dantzig1983reminiscences}. The simplex algorithm and its derivatives have since become ubiquitous in a number of disciplines including finance, engineering, transportation, and energy \cite{junger2010years}. These algorithms allow for the efficient solution of the global optimum of linear functions with an objective function, a number of variables and a number of constraints on these variables. However, the simplex algorithm is limited to problems where the \textit{feasible set} of possible solutions is convex.

To further increase the expressiveness of the linear programming \textit{language}, the inclusion of non-convex constraints such as limiting variables to only take integer values, is a reoccurring limitation in modeling real-world problems in a mathematical programming language. The set of optimization problems with these integer are known as \textit{Mixed Integer Linear Programs} (\gls{MILP}). The inclusion of integer constraints to these linear problems has proved to be a very challenging problem class to develop efficient solution algorithms for, as it is considered unlikely that a polynomial-time solution algorithm exists \cite{bengio2020machine}.

For problems including integer constraints, the na\"ive approach of comparing every possible combination of feasible solutions, known as \textit{explicit enumeration}, will result in a solution algorithm that is of exponential complexity, which makes larger optimization problems intractable. An alternative to explicit enumeration is \textit{implicit enumeration}, where a large number of possible solutions so not have to be evaluated explicitly. 
\textit{Branch and Bound} (\gls{BnB}), conceived in 1960 \cite{land1960automatic}, is an algorithm for solving mathematical programming problems via implicit enumeration. It has since received numerous improvements and extensions \cite{wolsey2020integer}. 

For time-constrained applications of these non-convex optimization problems, a decrease in time to calculate the optimal solution can result in significant improvements and has been a very active field of research for many decades \cite{wolsey2020integer}. For the interested reader, the modern advances of Branch and Cut, Column Generation, and Bender's Decomposition algorithms are recommended reading in \textit{Integer Programming} by Lawrence Wolsey \cite{wolsey2020integer}.

%Currently, the most efficient solution algorithms using Branch and Bound are proprietary algorithms, notably IBM CPLEX and Gurobi, however, open source solvers do exist and are under continuous development, e.g. \gls{COIN-OR} and \gls{SCIP} Optimization Suites \cite{achterberg2009scip,anand2017comparative}. Improvements to 

There is a large interest in both theoretical and practical mathematical programming for methods that can improve the efficiency of \gls{MILP} optimization algorithms \cite{wolsey2020integer}. This can have a lasting impact on the nature of mathematical programming and is therefore an important area to explore further. Researchers in Mathematical Optimization have also noted the potential for expert-constructed branching strategies based on knowledge obtained from \textit{Statistical Learning}, also known as \textit{Machine Learning} (\Gls{ML}) \cite{lodi2017learning}.


\subsection{Machine Learning}

The current dominating paradigm of \textit{Artificial Intelligence} is \textit{Machine Learning}, where computers (algorithms) learn from experience (data) \cite{goodfellow2016deep}. The capabilities of these models have had an exponential success in recent years, much due to advances in computer hardware \cite{goodfellow2016deep}. A variety of fields have seen breakthroughs by using methods in \gls{ML}, including medical diagnostics, industrial optimization, autonomous vehicles and board games \cite{goodfellow2016deep}. 

There are a number of sub-fields within Machine Learning. In this project, the class known as \textit{supervised classification} is explored. Supervised classification is the general problem of dividing instances into classes based on past instances and their respective classes. These models learn through observing examples of past data and their classifications. The basic assumption is that the \gls{ML} model will learn from its experience, and be accurate in classifying previously unseen data. 

A prevalent field within Machine Learning the last decade is \textit{Deep Learning} (DL), where models are built up of series of nonlinear functions \cite{goodfellow2016deep}. 
The Multi-Layer Perceptron, or feedforward deep network, is the most common model in Deep Learning. Nonlinear representations are iteratively performed, giving the model a large \textit{capacity} for representing complex relations between input and output. 

For many tasks, e.g. tasks with a real-time component, the strength of Machine Learning models lies in the fast evaluation of the generated, nonlinear functions, also known as the \textit{inference} \cite{bertsimas2019online}. This strength makes it possible for machines to take over tasks previously thought to require a human operator, or increase efficiency greatly via automation.

Further, the results of the iterative optimization process of generating an \gls{ML}-model can find patterns in data that are difficult to discover with traditional statistics \cite{goodfellow2016deep}. Much like chess Grandmasters learn from observing the best AI-players, so too might experts gain knowledge in their respective fields by analyzing the results of an algorithm built on statistical learning.  


\section{Previous Work}

There has been a recent surge of interest in leveraging machine learning methods in solving non-convex optimization problems, notably a recent literature review conducted by Yoshua Bengio \cite{bengio2020machine}. The aim is for statistical learning to aid in the efficient solving of complex problems without sacrificing the strong guarantees inherent in mathematical optimization solvers. These hybrid methods now show the potential to be competitive with the state-of-the-art solvers for these \gls{NP}-hard problems \cite{gasse2019exact}. 

An overview of the history of learning in \gls{BnB} is given a survey paper by Lodi and Zarpellon \cite{lodi2017learning}. To summarize, interest in using more advanced statistics to unravel the relations of a \gls{MILP} problem and the optimal branching variable was first observed in 2009. Various approaches to learning have been attempted, with recent efforts to directly incorporate the learned algorithms into the solution algorithm from 2016 and onwards \cite{lodi2017learning}.  

A thorough look into the possibilities of machine learning in \gls{BnB} was conducted by Elias Khalil \cite{khalil2020towards}, in which he chose the term \textit{data-driven algorithm design} for this approach. 
In \textit{Learning to Branch} (2016) \cite{khalil2016learning} imitation learning of Strong Branching was performed as a learn-to-rank-problem. The algorithm was competitive with a selection of modern solvers \cite{khalil2016learning}. 
Recent advances using\textit{ Graph Convolutional Neural Networks} (\Gls{GCNN}) have proved to consistently improve on the solution time of the best available open-source solvers by Gasse et al.  \cite{gasse2019exact}. 

The promising results found by Gasse et al. \cite{gasse2019exact} were, however, criticized by Gupta et al. \cite{gupta2020hybrid} for reliance on modern \gls{GPU} processing power. They showed that the efficiency of the GCNN-aided algorithm did not improve on the native branching strategies when run on a CPU. Gupta et al. presented novel methods running only on the CPU that were able to improve significantly on the native strategies. These methods include \textit{Support Vector Machines} (\Gls{SVM}), \textit{Multi-Layer Perceptrons} (\Gls{MLP}) and  \textit{Feature-wise Linear Modulation Models} (\gls{FiLM}) \cite{gupta2020hybrid}. 

Though GPU-aided algorithms are interesting in their own right, a fair comparison of algorithmic efficiency cannot be made when the machine learning-based models are aided by expensive, advanced and specialized GPU processing power, as in Gasse et al. \cite{gasse2019exact}. 
For this reason, all analysis of computational efficiency in this project will be performed on a CPU. Nevertheless, discrete optimization performed on GPUs is a very interesting topic, for a discussion on Branch and Bound algorithms on GPUs the reader is referred to Schultz et al. \cite{schulz2013gpu}. It is also assumed by the author that advances in ML-aided optimization can combine nicely with advances in parallel computing.

The methods of Gasse et al. \cite{gasse2019exact} and the improvements made by Gupta et al. \cite{gupta2020hybrid} have shown that data-driven methods can improve upon existing state-of-the-art solvers, and is therefore an avenue worth examining, exploring, and expanding further. Machine Learning is a rapidly developing field, and recent advances have shown to outclass the early proof-of-concept attempts, sometimes by considerable margins \cite{holzinger2018current}. There is little reason to believe that ML-leveraged algorithms do not have this latent potential. 

Recent attempts to facilitate the development of \gls{ML}-aided \gls{CO} includes a notable project named Extensible Combinatorial Optimization Learning Environments (\emph{Ecole}) \cite{prouvost2020ecole}. It was developed by the group Data Science For Decision Making (\gls{DS4DM}), connected to the Polytechnique Montr\'{e}al university. \gls{Ecole} is an open source framework for a controllable python interface to \gls{BnB} algorithms, and is built on SCIP and PySCIPOpt. The framework is tightly knit with the previous work from the group, notably Gasse et al. \cite{gasse2019exact} and Gupta et al. \cite{gupta2020hybrid}, and aims to standardize research within the field of \gls{BnB} algorithm improvement.  
As of May 2021, no articles have been published with results from using this framework. A recent article by
Cappart et al. (2021) \cite{cappart2021combinatorial} presents the current status of and the role of the \gls{Ecole} framework in further developing this field. 


\section{Research Questions}\label{sec:questions}

For this project, three research questions are considered.
%
\begin{enumerate}[label=(\roman*)]
    \item \textit{Can Graph Convolutional Neural Networks and  Multi-Layer Perceptrons decrease the running time of Branch and Bound running on the \Gls{CPU}?}
\end{enumerate}
%
A Multi-Layer Perceptron model is implemented by Gupta et al. \cite{gupta2020hybrid}, however it is used in the context of hybrid models, combining \Gls{GCNN} features with the general variable features. This is done to mitigate the loss in efficiency by running \gls{GCNN}s on the \gls{CPU}. It is unclear whether a purely \gls{MLP}-based approach can result in improvements on the native branching strategies. \gls{MLP}s have been the focus of research for many decades, and implementation is assumed to be relatively straightforward \cite{goodfellow2016deep}.
%
\begin{enumerate}[resume*]
    \item \textit{What is the impact of different \gls{GCNN} and \gls{MLP} model topologies on the accuracy and efficiency of the Branch and Bound algorithm, when run on the \gls{CPU} and \gls{GPU}}?
\end{enumerate}
%
Machine Learning model choice based on reduced complexity is cited as a motivation for design choices in Gupta et al. \cite{gupta2020hybrid}, however the impact of varying sizes of the same model has not been performed before, and the magnitude of the impact is unknown. Increased model capacity is known to facilitate higher accuracy \cite{goodfellow2016deep}, however, whether this has a detrimental effect on the Branch and Bound algorithm's overall performance is unknown. Results from methods that are more comparable will indicate the importance of accuracy versus computational complexity.   
%
\begin{enumerate}[resume*]
    \item \textit{What are the most promising research opportunities for learning in Branch and Bound?}
\end{enumerate}
%
The general field has gained traction recently \cite{bengio2020machine}, and the author assumes more research and interest will come in the next few years. In the implementation of this project, the new framework \gls{Ecole} is used. This is the first paper where \gls{Ecole} is used as the basis for the experiments, and an independent review of this framework is due.  



\section{Thesis Structure}

In the following chapter, \Cref{cha:background}, the necessary theoretical background in optimization and machine learning is presented, as well as a review of earlier work in the field. In \Cref{cha:methods}, the data set, chosen training and testing methods, and experiments are presented, along with the architectural choices. \Cref{cha:results} provides the results of the aforementioned experiments. Then, \Cref{cha:discussion} contains discussions of the results, a critique of the experiments, and ideas for further work. Finally, \Cref{cha:conclusion} summarizes the project with a conclusion on the implications of the results. 


