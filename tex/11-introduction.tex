%!TEX root = ../Thesis.tex
\chapter{Introduction}\label{cha:introduction}
%
This chapter presents a short history, motivation and background in the fields of mathematical programming and machine learning, summarizes the previous work on the topic of \textit{learning-to-branch}, poses the research questions for the project, and explains the structure of the project. \Cref{sec:background,sec:motivation,sec:previouswork} and \ref{sec:structure} are adapted from the project report \textit{Multi-layer Perceptrons for Branching in Mixed-Integer Linear Programming} (2020). 


\section{Background}\label{sec:background}

In this section, a relevant background in the relevant fields of mathematical programming and machine learning is presented, as well as an overview of the previous work in combining these fields. A familiarity of the reader with the central concepts and terms in this field is assumed.

\subsection{Mathematical Programming}

The invention of efficient solution algorithms to linear mathematical programming problems is considered one of the great post-war inventions \cite{dantzig1983reminiscences}. The simplex algorithm and its derivatives have since become ubiquitous in a number of disciplines including finance, engineering, transportation, and energy \cite{junger2010years}. These algorithms allow for the efficient solution of the global optimum of linear functions with an objective function, a number of variables and a number of constraints on these variables. However, the simplex algorithm is limited to problems where the \textit{feasible set} of possible solutions is convex.

To further increase the expressiveness of the linear programming \textit{language}, the inclusion of non-convex constraints such as limiting variables to only take integer values, is a reoccurring limitation in modeling real-world problems in a mathematical programming language. The set of optimization problems with these integer are known as \textit{Mixed Integer Linear Programs} (\gls{MILP}). The inclusion of integer constraints to these linear problems has proved to be a very challenging problem class to develop efficient solution algorithms for, as it is considered unlikely that a polynomial-time solution algorithm exists \cite{bengio2020machine}.

For problems including integer constraints, the na\"ive approach of comparing every possible combination of feasible solutions, known as \textit{explicit enumeration}, will result in a solution algorithm that is of exponential complexity, which makes larger optimization problems intractable. An alternative to explicit enumeration is \textit{implicit enumeration}, where a large number of possible solutions so not have to be evaluated explicitly. 
\textit{Branch and Bound} (\gls{BnB}), conceived in 1960 \cite{land1960automatic}, is an algorithm for solving mathematical programming problems via implicit enumeration. It has since received numerous improvements and extensions \cite{wolsey2020integer}. 

For time-constrained applications of these non-convex optimization problems, a decrease in time to calculate the optimal solution can result in significant improvements and has been a very active field of research for many decades \cite{wolsey2020integer}. For the interested reader, the modern advances of Branch and Cut, Column Generation, and Bender's Decomposition algorithms are recommended reading in \textit{Integer Programming} by Lawrence Wolsey \cite{wolsey2020integer}.

%Currently, the most efficient solution algorithms using Branch and Bound are proprietary algorithms, notably IBM CPLEX and Gurobi, however, open source solvers do exist and are under continuous development, e.g. \gls{COIN-OR} and \gls{SCIP} Optimization Suites \cite{achterberg2009scip,anand2017comparative}. Improvements to 

There is a large interest in both theoretical and practical mathematical programming for methods that can improve the efficiency of \gls{MILP} optimization algorithms \cite{wolsey2020integer}. This can have a lasting impact on the nature of mathematical programming and is therefore an important area to explore further. Researchers in Mathematical Optimization have also noted the potential for expert-constructed branching strategies based on knowledge obtained from \textit{Statistical Learning}, also known as \textit{Machine Learning} (\Gls{ML}) \cite{lodi2017learning}.


\subsection{Machine Learning}

The current dominating paradigm of \textit{Artificial Intelligence} is \textit{Machine Learning}, where computers (algorithms) learn from experience (data) \cite{goodfellow2016deep}. The capabilities of these models have had exponential success in recent years, much due to advances in computer hardware \cite{goodfellow2016deep}. A variety of fields have seen breakthroughs by using methods in \gls{ML}, including medical diagnostics, industrial optimization, autonomous vehicles, and board games \cite{goodfellow2016deep}. 

There are a number of sub-fields within Machine Learning. In this project, the class known as \textit{supervised classification} is explored. Supervised classification is the general problem of dividing instances into classes based on past instances and their respective classes. These models learn through observing examples of past data and their classifications. The basic assumption is that the \gls{ML} model will learn from its experience, and be accurate in classifying previously unseen data. 

A prevalent field within Machine Learning in the last decade is \textit{Deep Learning} (DL), where models are built up of series of nonlinear functions \cite{goodfellow2016deep}. 
The Multi-Layer Perceptron, or feedforward deep network, is the most common model in Deep Learning. Nonlinear representations are iteratively performed, giving the model a large \textit{capacity} for representing complex relations between input and output. 

For many tasks, e.g. tasks with a real-time component, the strength of Machine Learning models lies in the fast evaluation of the generated, nonlinear functions, also known as the \textit{inference} \cite{bertsimas2019online}. This strength makes it possible for machines to take over tasks previously thought to require a human operator or increase efficiency greatly via automation.

Further, the results of the iterative optimization process of generating an \gls{ML}-model can find patterns in data that are difficult to discover with traditional statistics \cite{goodfellow2016deep}. Much like chess Grandmasters learn from observing the best \gls{AI} players, so too might experts gain knowledge in their respective fields by analyzing the results of an algorithm built on statistical learning.  



\section{Previous Work}\label{sec:previouswork}

There has been a recent surge of interest in leveraging machine learning methods in solving non-convex optimization problems, notably a recent literature review conducted by Yoshua Bengio \cite{bengio2020machine}. The aim is for statistical learning to aid in the efficient solving of complex problems without sacrificing the strong guarantees inherent in mathematical optimization solvers. These hybrid methods now show the potential to be competitive with the state-of-the-art solvers for these \gls{NP}-hard problems \cite{gasse2019exact}. 

An overview of the history of learning in \gls{BnB} is given a survey paper by Lodi and Zarpellon \cite{lodi2017learning}. To summarize, interest in using more advanced statistics to unravel the relations of a \gls{MILP} problem and the optimal branching variable was first observed in 2009. Various approaches to learning have been attempted, with recent efforts to directly incorporate the learned algorithms into the solution algorithm from 2016 and onwards \cite{lodi2017learning}.  

A thorough look into the possibilities of machine learning in \gls{BnB} was conducted by Elias Khalil \cite{khalil2020towards}, in which he chose the term \textit{data-driven algorithm design} for this approach. 
In \textit{Learning to Branch} (2016) \cite{khalil2016learning} imitation learning of Strong Branching was performed as a learn-to-rank problem. The algorithm was competitive with a selection of modern solvers \cite{khalil2016learning}. 
Recent advances using\textit{ Graph Convolutional Neural Networks} (\Gls{GCNN}) have proved to consistently improve on the solution time of the best available open-source solvers by Gasse et al. (2019) \cite{gasse2019exact}. 

The promising results found by Gasse et al. (2019) \cite{gasse2019exact} were, however, criticized by Gupta et al. (2020) \cite{gupta2020hybrid} for reliance on modern \gls{GPU} processing power. They showed that the efficiency of the \gls{GCNN}-aided algorithm did not improve on the native branching strategies when run on a \gls{CPU}. Gupta et al. presented novel methods running only on the \gls{CPU} that were able to improve on the native strategies. These methods include \textit{Support Vector Machines} (\Gls{SVM}), \textit{Multi-Layer Perceptrons} (\Gls{MLP}) and  \textit{Feature-wise Linear Modulation Models} (\gls{FiLM}) \cite{gupta2020hybrid}. 

Though \gls{GPU}-aided algorithms are interesting in their own right, a fair comparison of algorithmic efficiency cannot be made when the machine learning-based models are aided by expensive, advanced and specialized \gls{GPU} processing power, as in Gasse et al. (2019) \cite{gasse2019exact}. However, the efficiency when run on a \gls{GPU} will be interesting for applications where this is available. 
For this reason, all analysis of computational efficiency in this project will be performed on both \gls{GPU} and \gls{CPU}. Discrete optimization performed on \gls{GPU}s is a very interesting topic, for a discussion on Branch and Bound algorithms on \gls{GPU}s the reader is referred to Schultz et al. \cite{schulz2013gpu}. It is also assumed by the author that advances in ML-aided optimization can combine nicely with advances in parallel computing.

The methods of Gasse et al. (2019) \cite{gasse2019exact} and the improvements made by Gupta et al. (2020) \cite{gupta2020hybrid} have shown that data-driven methods can improve upon existing state-of-the-art solvers, and is therefore an avenue worth examining, exploring, and expanding further. Machine Learning is a rapidly developing field, and recent advances have shown to outclass the early proof-of-concept attempts, sometimes by considerable margins \cite{holzinger2018current}. There is little reason to believe that \gls{ML}-leveraged algorithms do not have this latent potential. Further, many projects on implementing \gls{RL} in \gls{BnB} \cite{tang2020reinforcement,etheve2020reinforcement} an understanding of the relevant Neural Network models is fundamental. 

Recent attempts to facilitate the development of \gls{ML}-aided \gls{BnB} includes a notable project named \textit{Extensible Combinatorial Optimization Learning Environments} (\emph{Ecole}) \cite{prouvost2020ecole, prouvost2021ecole}. It was developed by the group \textit{Data Science For Decision Making} (\gls{DS4DM}), connected to the Polytechnique Montr\'{e}al university. \gls{Ecole} is an open-source framework for a controllable and extensible python interface to \gls{BnB} algorithms and is built on \gls{SCIP} and PySCIPOpt. The framework is tightly knit with the previous work from the group, notably Gasse et al. (2019) \cite{gasse2019exact} and Gupta et al. (2020) \cite{gupta2020hybrid}, and aims to standardize research within the field of \gls{BnB} algorithm improvement.  
 A recent article by
Cappart et al. (2021) \cite{cappart2021combinatorial} presents the current status of and the role of the \gls{Ecole} framework in further developing this field. 
As of May 2021, no articles have been published with results from using this framework.



\section{Motivation}\label{sec:motivation}

As stated, the ubiquity of \gls{BnB} algorithms in industry implies that increased efficiency in computation time will be very beneficial. The benefits could be in the form of reduced resource expenditure on computations, increased time resolution in real-time applications, or even new applications due to the increased efficiency.  

Machine Learning has been proved by many researchers to be a good candidate for improving \gls{BnB} \cite{khalil2016learning,gasse2019exact,gupta2020hybrid,khalil2020towards,etheve2020reinforcement}. Of these, the Graph Convolutional Neural Network presented in Gasse et al. (2019) \cite{gasse2019exact} has received the most attention and provides a very interesting approach to the variable selection problem in \gls{BnB}. The model showed satisfactory improvements over the \gls{SCIP} native brancher, however, the model was shown to be non-competitive when running on a \gls{CPU} in Gupta et al. (2020). 

While the alternative models presented in Gupta et al. (2020) \cite{gupta2020hybrid} are interesting, a thorough exploration of the more standard models in \gls{ML} (\gls{GCNN}s and \gls{MLP}s) is assumed to be more fruitful if they can achieve similar performance. The \gls{GCNN} developed and presented in Gasse et al. (2019) (from now on referred to as the \textit{Gasse \gls{GCNN}}) might not be competitive on the \gls{CPU} in the current configuration, however, this does not mean that the \gls{GCNN} architecture is unsuited for the application on a \gls{CPU}.

A comparison, with the solution efficiency reported on both \gls{GPU} and \gls{CPU}, of a selection of \textit{iterative ablations} on the Gasse \gls{GCNN} can give further insight into the model accuracy versus solution efficiency trade-off. Hopefully, this can give a more definitive answer to whether the \gls{GCNN} models are limited to purely \gls{GPU} applications. 

The term \textit{iterative ablations} (\Gls{IA}) will be used to describe the process of removing parts of a network sequentially. The term is, to the knowledge of the author, only used once before, in the context of ablative liver surgery in Seror (2015) \cite{seror2015ablative}. 

In addition, the choice to implement the source code in the new \gls{Ecole} framework is a conscious decision to aid in the standardization of how research is done within the field of improving \gls{BnB} algorithms.  


\section{Research Questions}\label{sec:questions}

For this project, three research questions are considered. This is done to aid the reader in following the thesis. The questions will be answered explicitly in \Cref{cha:discussion}. In addition, the topics of accuracy-efficiency-implementation will be reoccurring throughout the chapters of the thesis. 

The research questions are as follows:
%
\begin{enumerate}[label=(\roman*)]
    \item \textit{What is the impact on the accuracy of a \gls{GCNN} by iterative ablation?}
\end{enumerate}
%
The \gls{GCNN} successfully implemented in Gasse et al. (2019) \cite{gasse2019exact} was criticized in Gupta et al. (2020) for not being competitive with the classical strategies when run on a \gls{CPU}.
Hybrid models were implemented in Gupta et al. (2019) \cite{gupta2020hybrid}, where \Gls{GCNN} features were combined with the general variable features. This is done to mitigate the loss in efficiency by running \gls{GCNN}s on the \gls{CPU}. To investigate whether the original model can be competitive on both the \gls{GPU} and \gls{CPU}, 5 models are constructed by iteratively reducing the model size. These models will be referred to as GNN2, GNN1, MLP3, MLP2, and MLP1. The last models will consist of Multi-Layer Perceptrons with only the variable features, and will therefore have dissimilar names. The \gls{MLP} models are devised based on positive results found in the project \textit{Multi-Layer Perceptrons for Learning to Branch
} (2020).
%
\begin{enumerate}[resume*]
    \item \textit{What is the change in the efficiency of the iteratively ablated \gls{GCNN} when run on the \gls{CPU} and \gls{GPU} as a part of the \gls{BnB} algorithm}?
\end{enumerate}
%
Machine Learning model choice based on reduced complexity is cited as a motivation for design choices in Gupta et al. \cite{gupta2020hybrid}, however the impact of varying sizes of the same model has not been performed before, and the magnitude of the impact is unknown. Increased model capacity is known to facilitate higher accuracy \cite{goodfellow2016deep}, however, whether this has a detrimental effect on the Branch and Bound algorithm's overall performance is unknown. Results from methods that are more comparable will indicate the importance of accuracy versus computational complexity. It is also not clear whether the graph convolution as developed in the Gasse \gls{GCNN} is the component resulting in a detrimental loss of \gls{CPU} performance. By conducting all experiments on both types of hardware, results should be sufficient to conclude with a degree of certainty what the accuracy and efficiency trade-offs are and what differences there are between the \gls{CPU} and \gls{GPU} methods. 
%
\begin{enumerate}[resume*]
    \item \textit{What are the most promising research opportunities for learning in Branch and Bound?}
\end{enumerate}
%
The general field has gained traction recently \cite{bengio2020machine}, and the author assumes more research and interest will come in the next few years. In the implementation of this project, the new framework \gls{Ecole} is used. This is the first paper where \gls{Ecole} is used as the basis for the experiments, and an independent review of this framework is due. The road towards a conform and standardized framework and practice to develop methods in the field of \gls{ML} aided \Gls{BnB} can increase productivity and usher in breakthroughs in this field. On a broader note, as there is so much interest in the field, providing ideas for further research appears highly relevant. 



\section{Thesis Structure}\label{sec:structure}


\Cref{cha:introduction} contains an introduction to the thesis with a short background in the relevant field, an overview of previous work, a section on the motivation for the conducted experiments, a formulation of the three research questions, and an overview of the thesis structure. 
In the following chapter, \Cref{cha:background}, the necessary theoretical background in optimization and machine learning is presented, as well as a review of earlier work in the field. In \Cref{cha:methods}, the data set, chosen training and testing methods, and experiments are presented, along with the architectural choices. \Cref{cha:results} provides the results of the aforementioned experiments. Then, \Cref{cha:discussion} contains discussions of the results, a critique of the experiments, and ideas for further work. Finally, \Cref{cha:conclusion} summarizes the project with a conclusion on the implications of the results. 


of 