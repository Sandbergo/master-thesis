\chapter{Conclusion}\label{cha:conclusion}

In this thesis, the Graph Convolutional Neural Network presented in Gasse et al. (2019) \cite{gasse2019exact} was iteratively ablated. The ablations resulted in five models, among which two were graph convolutional neural networks and three were pure multi-layer perceptrons. The \gls{GCNN} models used the bipartite graph nature of the constraint-variable relationship, while the \gls{MLP}s only used the features of the candidate variables. The models were trained, tested, and evaluated on generated \gls{MILP} problems from the problem classes combinatorial auctions and set covering. All resulting models were tested for accuracy on predicting the optimal branching variable according to the Strong Branching algorithm. The efficiency of the \gls{ML}-enhanced solvers were evaluated by running the models on both a \gls{GPU} and \gls{CPU}. These experiments were chosen in order to gain insight into the model and help future researchers make informed choices on \gls{ML} model selection. 

% Ecole
The experiments were implemented in the new framework \textit{\gls{Ecole}} \cite{prouvost2020ecole}. The framework provides an interface for the \gls{BnB} solver \gls{SCIP}, inspired by \textit{OpenAI Gym} \cite{brockman2016openai}.  
This thesis is the first article to use \gls{Ecole} except for the introductory papers by Provoust et al. (2020) \cite{prouvost2020ecole} and Cappart et al. (2021) \cite{cappart2021combinatorial}. The framework was evaluated to be useful, especially as it is improved upon in the future.

% accuracy ...
The accuracy of the models consistently decreased as layers were removed from the original model. There was a significant loss of accuracy after removing the graph convolutional component as well as after removing all hidden layers of the model. Both problem sets showed the same tendency, though the degradation of accuracy was more dramatic for the set covering problems, where there is a larger number of constraints.  
%The iterative ablations showed a consistent decrease in accuracy for both problem classes. The notable decreases in accuracy occurred after removing the graph convolutional modules (resulting in a pure Multi-Layer Perceptron) and after all of the hidden layers of the model were removed, resulting in a linear model. A larger loss of accuracy was found for the set covering problems than the combinatorial auction problems after the removal of the graph convolutions. This is assumed to be because of the higher number of constraints in the set covering problems compared to the combinatorial auction problems. 

% efficiency ...
The computation time per variable decision also decreased with the ablations. Significant reductions in time per node were found after removing the graph convolutions and after removing all hidden layers. When the \gls{SCIP} solver was run on test problems by performing the variable selection with the learned models, all models showed competitive efficiency with a selection of classical branching algorithms. When the models were run on the \gls{CPU}, the more complex models suffered a large loss of efficiency. This particularly affected the models containing graph convolutions. Results were also indicative of the problem formulation being relevant for the viability of running \gls{GCNN} models on the \gls{CPU}. 
%All ablations resulted in decreased computation time per variable branching decision. All models were competitive with the classical branching strategies (Full Strong branching, Pseudo-cost branching, Reliability Pseudo-cost branching) when run on the \gls{GPU}. On the \gls{CPU}, there was a significant decrease in performance for the models containing graph convolutions. This performance reduction was greater for the set covering problems, indicating that the number of constraints might exacerbate problems with running graph convolutional models on restricted hardware. Reducing the number of hidden layers did not impact the computation time per node in particular, and is therefore not considered a viable alternative for reducing the computation time.    

% Implications ...
The main implications of the results are the importance of considering the hardware the \gls{ML} enhanced solver will be deployed on, as well as the problem types the models are tested on. The relation between the running times of the different models on both hardware implied non-trivial relations, which urges caution for future attempts at developing \gls{ML} models for this purpose. The results in this thesis as well as the project \textit{Multi-layer Perceptrons for Branching in Mixed-Integer Linear Programming} (2020) and the article by Gupta et al. (2020) \cite{gupta2020hybrid} stipulate a shift toward less computationally complex models with richer input features (observation functions). These models will be more universally applicable and predictable on the various hardware that will run \gls{BnB} algorithms. In addition, the most recent attempt at learning to branch by Zarpellon et al. (2020) \cite{zarpellon2020parameterizing} is consistent with this observation by training models that generalize across problem classes.

% Future work ...
Future work should take into account the implications of this thesis in terms of both hardware and problem class. The analysis of how problem formulations can be detrimental to model performance is also highly relevant and should be taken into consideration when presenting data-driven methods with the implication of universally useful. Following the trend of the last few years, models pre-trained with imitation learning and improved with reinforcement learning can provide further advances in the field of \gls{ML} enhanced \gls{BnB}. These models will yield insights into the greater topic of machine-created algorithms, and may revolutionize how the hardest computational problems are solved in the future.  

%\begin{chapquote}{Thomas Aquinas, \textit{Summa Theologica}}
%``Quod potest compleri per pauciora principia, non fit per plura.\footnote{It is superfluous %to suppose that what can be accounted for by a few principles has been produced by many.}''
%\end{chapquote}

% Diskusjonskapittel
% Rød tråd: Accuracy vs. efficiency